{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2025 Symbolic regression Monod paper\n",
    "# Pretreating real data and Generating simulated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = \"/scratch/project_2000746/anthosun/2025SRMO/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# smoothing\n",
    "\n",
    "halfwindow = 3\n",
    "\n",
    "def summingaround(N, halfwindow = 3):\n",
    "\n",
    "    AA = N.copy()\n",
    "    \n",
    "    for w in np.arange(halfwindow) + 1: # summing over (2*halfwindow)-window\n",
    "        BB = np.roll(AA, shift=-w, axis=0)\n",
    "        BB[-w:] = 0\n",
    "        AA = AA + BB\n",
    "        BB = np.roll(AA, shift=w, axis=0)\n",
    "        BB[:w] = 0\n",
    "        AA = AA + BB\n",
    "    \n",
    "    return AA\n",
    "\n",
    "\n",
    "def smoothing(N, halfwindow = 3):\n",
    "    \n",
    "    AA = summingaround(N, halfwindow=halfwindow)\n",
    "    BB = np.ones(AA.shape)\n",
    "    BB = summingaround(BB, halfwindow=halfwindow)\n",
    "    \n",
    "    return AA / BB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute per-capita derivatives\n",
    "\n",
    "def to_rhos(T, N):\n",
    "    \n",
    "    if len(T.shape) == 1:\n",
    "        assert T.shape[0] == N.shape[0], \"T-array has length {} but N-array has length {}!\".format(T.shape[0], N.shape[0])\n",
    "        for axis in N.shape[1:]:\n",
    "            T = T[...,np.newaxis]\n",
    "    else:\n",
    "        assert T.shape == N.shape, \"T-array has shape {} but N-array has shape {}!\".format(T.shape, N.shape)\n",
    "    \n",
    "    if T.dtype == int:\n",
    "        T = T.astype(float)\n",
    "    \n",
    "    # delta N / N\n",
    "    \n",
    "    # forward roll for N\n",
    "    A = np.roll(N, shift=-1, axis=0)\n",
    "    A[-1:] = np.nan\n",
    "    A = A - N\n",
    "    A[np.isnan(A)] = 0\n",
    "\n",
    "    # backward roll for N\n",
    "    B = np.roll(N, shift=1, axis=0)\n",
    "    B[:1] = np.nan\n",
    "    B = B - N\n",
    "    B[np.isnan(B)] = 0\n",
    "    \n",
    "    NNN = N.copy()\n",
    "    NNN[N!=0] = (A[N!=0] - B[N!=0]) / N[N!=0]\n",
    "\n",
    "    # delta T\n",
    "    \n",
    "    # forward roll for T\n",
    "    A = np.roll(T, shift=-1, axis=0)\n",
    "    A[-1:] = np.nan\n",
    "    A = A - T\n",
    "    A[np.isnan(A)] = 0\n",
    "\n",
    "    # backward roll for T\n",
    "    B = np.roll(T, shift=1, axis=0)\n",
    "    B[:1] = np.nan\n",
    "    B = B - T\n",
    "    B[np.isnan(B)] = 0\n",
    "\n",
    "    T = A - B\n",
    "\n",
    "    return NNN / T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute AUCs/integrals\n",
    "# returns an array of same dimensions,\n",
    "# with first time point being zero\n",
    "\n",
    "def to_AUCs(T, N, t0 = 0 # t0: if integer, reference index for integral computation, otherwise the raw AUC is returned\n",
    "           ):\n",
    "    \n",
    "    if len(T.shape) == 1:\n",
    "        assert T.shape[0] == N.shape[0], \"T-array has length {} but N-array has length {}!\".format(T.shape[0], N.shape[0])\n",
    "        for axis in N.shape[1:]:\n",
    "            T = T[...,np.newaxis]\n",
    "    else:\n",
    "        assert T.shape == N.shape, \"T-array has shape {} but N-array has shape {}!\".format(T.shape, N.shape)\n",
    "    \n",
    "    A = ( np.roll(N, shift=1, axis=0) + N ) / 2 # average N\n",
    "#    A[0] = N[0] # not useful since replaced immediately\n",
    "\n",
    "    B = T - np.roll(T, shift=1, axis=0) # delta T\n",
    "    \n",
    "    A *= B # average N * delta T\n",
    "    A[0] = 0 # cut off first value, # Flooring is not required when using trapezoidal rule\n",
    "    \n",
    "    B = np.cumsum(A, axis=0) # cumulative sum\n",
    "    \n",
    "    return B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretreat real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "\n",
    "for name in listdir(\"{}/raws\".format(cwd)):\n",
    "\n",
    "    if name[:3] != \"Rs0\":\n",
    "        continue\n",
    "\n",
    "    df = pd.read_csv(\"{}/raws/{}\".format(cwd, name), index_col=[0,1] )\n",
    "    data = pd.DataFrame()\n",
    "    concs = sorted(list(df.index.get_level_values(\"conc\").unique()))\n",
    "    \n",
    "    for conc in concs:\n",
    "        \n",
    "        N = df.xs(conc, level=\"conc\")\n",
    "        N = N.sort_index()\n",
    "\n",
    "        T = np.array(N.index, dtype=float) / 60 / 60 # convert time unit from seconds to hours\n",
    "        N = np.mean(N.to_numpy(), axis=1) # mean across replicates\n",
    "        N = smoothing(N) # smoothing\n",
    "        Nc = to_AUCs(T, N, t0=0)\n",
    "        F = np.exp(-Nc)\n",
    "        R = to_rhos(T, N)\n",
    "\n",
    "        dataframe = pd.DataFrame()\n",
    "        dataframe[\"T\"] = T\n",
    "        dataframe[\"n\"] = N\n",
    "        dataframe[\"Nc\"] = Nc\n",
    "        dataframe[\"F\"] = F\n",
    "        dataframe[\"U\"] = np.exp(- T)\n",
    "        dataframe[\"rho\"] = R\n",
    "        dataframe[\"C\"] = conc\n",
    "\n",
    "        data = pd.concat([data, dataframe])\n",
    "        \n",
    "    data.to_csv(\"{}/data/{}\".format(cwd, name), sep=\",\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation Fig.S1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code = \"R\"\n",
    "resc = \"0\"\n",
    "\n",
    "freqorder = list(pd.read_csv(\"/scratch/project_2000746/anthosun/2025SRMO/raws/Frequencies.csv\", sep=\",\", index_col=0,).index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nplots = 2 # nb of mini-plots per strain\n",
    "nranks = 2 # nb of meta-columns\n",
    "\n",
    "YYY = len(freqorder) // nranks\n",
    "XXX = nplots * nranks\n",
    "\n",
    "cmap = mpl.colormaps[\"viridis\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,8), constrained_layout=True)\n",
    "fig.supxlabel(\"time\")\n",
    "fig.supylabel(\"bacterial species (HAMBI code)\")\n",
    "\n",
    "gs = fig.add_gridspec(YYY, XXX) # (y, x)\n",
    "axs = gs.subplots()\n",
    "\n",
    "for i, strn in enumerate(freqorder): # go through strains\n",
    "    strn = str(strn)\n",
    "    rank = i // YYY\n",
    "    row = i % YYY\n",
    "    \n",
    "    data = pd.read_csv(\"{}/data/{}s{}i{}.csv\".format(cwd, code, resc, strn), sep=\",\")\n",
    "    concs = sorted(list(data[\"C\"].apply(float).unique()))\n",
    "\n",
    "    for k, y in enumerate([\"n\", \"rho\"]):\n",
    "        ax = axs[row,nplots*rank+k]\n",
    "        \n",
    "        for c, conc in enumerate(concs):\n",
    "            colour = cmap(c / len(concs))\n",
    "    \n",
    "            datc = data[data[\"C\"] == conc]\n",
    "            xs = [datc[\"T\"].to_numpy(), datc[\"T\"].to_numpy(), np.arange(0, 5000, 1)]\n",
    "            ax.plot(xs[k], datc[y], c=colour)\n",
    "            \n",
    "        ax.tick_params(axis=\"both\", which=\"both\", bottom=False, top=False, left=False, right=False, labelbottom=False, labeltop=False, labelleft=False, labelright=False, )\n",
    "        ax.set(xlim = (0, np.max(xs[k])), #ylim = (0, data[y].max() * 1.1),\n",
    "               ylabel = str(strn) if k == 0 else None,\n",
    "               title = [r\"population size $N$\", r\"per capita growth rate $\\rho_{\\rm obs}$\", ][k] if row == 0 else None, )\n",
    "\n",
    "### SAVING\n",
    "name = \"Time_dependence_{}s{}.png\".format(code, resc)\n",
    "plt.savefig(\"{}/plot/{}\".format(cwd, name), facecolor='w', edgecolor='w', transparent=False, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulated data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load simulation parameters from /cwd/pars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concs = np.genfromtxt(\"{}/pars/concs.csv\".format(cwd,), delimiter=\",\")\n",
    "nuxi = np.genfromtxt(\"{}/pars/nuxi.csv\".format(cwd,), delimiter=\",\")\n",
    "K_ix = np.genfromtxt(\"{}/pars/K_ix.csv\".format(cwd,), delimiter=\",\")\n",
    "gmix = np.genfromtxt(\"{}/pars/gmix.csv\".format(cwd,), delimiter=\",\")\n",
    "qm_i = np.genfromtxt(\"{}/pars/qm_i.csv\".format(cwd,), delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nconcs = len(concs)\n",
    "nrescs, nstrns = nuxi.shape\n",
    "\n",
    "assert K_ix.shape == (nstrns, nrescs)\n",
    "assert gmix.shape == (nstrns, nrescs)\n",
    "assert qm_i.shape == (2, nstrns)\n",
    "\n",
    "rescs = np.arange(nrescs) + 1\n",
    "strns = np.arange(nstrns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamical system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resource-consumer model for simulation\n",
    "\n",
    "def RCM(t, X, substrate_nb, growth_function, nuxi, *args):\n",
    "    \n",
    "    S, N = X[:substrate_nb], X[substrate_nb:]\n",
    "    dX = np.zeros(X.shape)\n",
    "    dX[:substrate_nb] = (nuxi @ N) * S # dynamics of the substrates\n",
    "    dX[substrate_nb:] = growth_function(S, *args) * N # dynamics of the population\n",
    "\n",
    "    return dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjustment function\n",
    "\n",
    "def alpha_RCM(t, q, m):\n",
    "    \n",
    "    alpha = q / (q + np.exp(- m * t))\n",
    "\n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Growth functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model A (additive with global g_max)\n",
    "\n",
    "def fA(S,    # (        nrescs)\n",
    "       gmix, # (nstrns, nrescs)\n",
    "       K_ix, # (nstrns, nrescs)\n",
    "      ):\n",
    "    \n",
    "    F = S[np.newaxis,:] / ( K_ix + S[np.newaxis,:] ) # (nstrns, nrescs)\n",
    "    F = gmix[:,0] * np.sum(F, axis = -1) # (nstrns)\n",
    "    \n",
    "    return F # (nstrns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model M (multiplicative)\n",
    "\n",
    "def fM(S, gmix, K_ix):\n",
    "    \n",
    "    F = S[np.newaxis,:] / ( K_ix + S[np.newaxis,:] )\n",
    "    F = gmix[:,0] * np.prod(F, axis = -1)\n",
    "    \n",
    "    return F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation and saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulation time\n",
    "\n",
    "dt = 0.1\n",
    "tmax = 50000\n",
    "subtime = 500 # timepoint subsampling: 1/500\n",
    "\n",
    "ts = np.arange(0, tmax + dt, dt) # timepoints for simulation\n",
    "subts = ts[::subtime,np.newaxis]\n",
    "T = np.repeat(subts, nconcs, axis=-1) # # timepoints for sub-sampling (ntimes, nconcs)\n",
    "N = np.zeros(T.shape) # sub-sampled population sizes (ntimes, nconcs)\n",
    "\n",
    "ntimes = ts.shape[0]\n",
    "nsbtms = T.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# simulation\n",
    "\n",
    "for resc in rescs:\n",
    "    \n",
    "    nu = nuxi[:resc,:]\n",
    "    gm = gmix[:,:resc]\n",
    "    K_ = K_ix[:,:resc]\n",
    "    \n",
    "    for modl in [fA, fM]:\n",
    "        for strn in strns[:1]:\n",
    "                \n",
    "            name = \"{}s{}i{}.csv\".format(modl.__name__[1], resc, strn)\n",
    "            \n",
    "            for c, conc in enumerate(concs):\n",
    "                print(name, c, \"   \", end=\"\\r\")\n",
    "                \n",
    "                # intial condition\n",
    "                S0 = np.ones(resc) * 0.1 * conc #/ (resc)\n",
    "                N0 = np.zeros(nstrns)\n",
    "                N0[strn] = 0.1\n",
    "                X0 = np.concatenate([S0, N0])\n",
    "                \n",
    "                # Euler's method\n",
    "                for t, time in enumerate(ts):\n",
    "                    dX = RCM(t, X0, resc, modl, nu, gm, K_,) * dt # Euler step\n",
    "                    dX[resc:] *= alpha_RCM(time, qm_i[0], qm_i[1]) # adjustment function alpha\n",
    "                    X0 += dX\n",
    "                    # record only if the time is to be kept in subtimes Ts\n",
    "                    if time in subts:\n",
    "                        N[np.argmax(subts==time),c] = np.sum(X0[resc:]) # (ntimes, nconcs)\n",
    "            \n",
    "            Nc = to_AUCs(T, N, t0=0)\n",
    "            F = np.exp(-Nc)\n",
    "            R = to_rhos(T, N)\n",
    "            \n",
    "            df = pd.DataFrame()\n",
    "            df[\"t\"] =   T.flatten(order=\"F\")\n",
    "            df[\"N\"] =   N.flatten(order=\"F\")\n",
    "            df[\"Nc\"] = Nc.flatten(order=\"F\")\n",
    "            df[\"F\"] =   F.flatten(order=\"F\")\n",
    "            df[\"rho\"] = R.flatten(order=\"F\")\n",
    "            df[\"C\"] = np.repeat(concs, nsbtms)\n",
    "            \n",
    "            df.to_csv(\"{}/data/{}\".format(cwd, name), sep=\",\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add lfit-inferred adjustment function alpha as a new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "\n",
    "files = sorted(listdir(\"{}/data\".format(cwd)))\n",
    "lfit_results = pd.read_csv(\"{}/lfit/lfit_results.csv\".format(cwd), sep=\",\", header=[0,1,2,3], index_col=[0], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files:\n",
    "    \n",
    "    name = file[:file.index(\".csv\")]\n",
    "    code = name[:name.index(\"s\")]\n",
    "    resc = name[name.index(\"s\")+1:name.index(\"i\")]\n",
    "    strn = name[name.index(\"i\")+1:]\n",
    "    \n",
    "    data = pd.read_csv(\"{}/data/{}\".format(cwd, file), sep=\",\")\n",
    "    data[\"inferred_alpha\"] = data[\"C\"].apply(lambda x: lfit_results.loc[\"q\",(code,resc,strn,str(x))] ) # parameter q\n",
    "    data[\"m\"] = data[\"C\"].apply(lambda x: lfit_results.loc[\"m\",(code,resc,strn,str(x))] )\n",
    "    data[\"inferred_alpha\"] = data[\"inferred_alpha\"] / (np.exp(- data[\"m\"] * data[\"t\"]) + data[\"inferred_alpha\"] )\n",
    "    data.drop(\"m\", axis=1, inplace=True)\n",
    "\n",
    "    data.to_csv(\"{}/data/{}\".format(cwd, file), sep=\",\", index=False)\n",
    "    print(name, \"   \", end=\"\\r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add lfit-inferred single adjustment function alpha\n",
    "# (one across all concentrations) as a new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "\n",
    "files = sorted(listdir(\"{}/data\".format(cwd)))\n",
    "lfit_results = pd.read_csv(\"{}/lfit/lfit_results_single_alpha.csv\".format(cwd), index_col=[0], header=[0,1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files:\n",
    "    \n",
    "    name = file[:file.index(\".csv\")]\n",
    "    code = name[:name.index(\"s\")]\n",
    "    resc = name[name.index(\"s\")+1:name.index(\"i\")]\n",
    "    strn = name[name.index(\"i\")+1:]\n",
    "    \n",
    "    data = pd.read_csv(\"{}/data/{}\".format(cwd, file), sep=\",\")\n",
    "    q = lfit_results.loc[\"q\",(code,resc,strn)] # parameter q\n",
    "    m = lfit_results.loc[\"m\",(code,resc,strn)] # parameter q\n",
    "    data[\"inferred_single_alpha\"] = q / (np.exp(- m * data[\"t\"]) + q )\n",
    "\n",
    "    data.to_csv(\"{}/data/{}\".format(cwd, file), sep=\",\", index=False)\n",
    "    print(name, \"   \", end=\"\\r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation and saving of no-adjustment data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulation time\n",
    "\n",
    "dt = 0.1\n",
    "tmax = 50000\n",
    "subtime = 500 # timepoint subsampling: 1/500\n",
    "\n",
    "ts = np.arange(0, tmax + dt, dt) # timepoints for simulation\n",
    "subts = ts[::subtime,np.newaxis]\n",
    "T = np.repeat(subts, nconcs, axis=-1) # # timepoints for sub-sampling (ntimes, nconcs)\n",
    "N = np.zeros(T.shape) # sub-sampled population sizes (ntimes, nconcs)\n",
    "\n",
    "ntimes = ts.shape[0]\n",
    "nsbtms = T.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulation\n",
    "\n",
    "for resc in rescs[-1:]:\n",
    "    \n",
    "    nu = nuxi[:resc,:]\n",
    "    gm = gmix[:,:resc]\n",
    "    K_ = K_ix[:,:resc]\n",
    "    \n",
    "    for modl in [fA, fM][-1:]:\n",
    "        for strn in strns:\n",
    "                \n",
    "            name = \"{}s{}i{}.csv\".format(modl.__name__[1], resc, strn)\n",
    "            \n",
    "            for c, conc in enumerate(concs):\n",
    "                print(name, c, \"   \", end=\"\\r\")\n",
    "                \n",
    "                # intial condition\n",
    "                S0 = np.ones(resc) * 0.1 * conc #/ (resc)\n",
    "                N0 = np.zeros(nstrns)\n",
    "                N0[strn] = 0.1\n",
    "                X0 = np.concatenate([S0, N0])\n",
    "                \n",
    "                # Euler's method\n",
    "                for t, time in enumerate(ts):\n",
    "                    dX = RCM(t, X0, resc, modl, nu, gm, K_,) * dt # Euler step\n",
    "#                    dX[resc:] *= alpha_RCM(time, qm_i[0], qm_i[1]) # adjustment function alpha\n",
    "                    X0 += dX\n",
    "                    # record only if the time is to be kept in subtimes Ts\n",
    "                    if time in subts:\n",
    "                        N[np.argmax(subts==time),c] = np.sum(X0[resc:]) # (ntimes, nconcs)\n",
    "            \n",
    "            Nc = to_AUCs(T, N, t0=0)\n",
    "            F = np.exp(-Nc)\n",
    "            R = to_rhos(T, N)\n",
    "            \n",
    "            df = pd.DataFrame()\n",
    "            df[\"t\"] =   T.flatten(order=\"F\")\n",
    "            df[\"N\"] =   N.flatten(order=\"F\")\n",
    "            df[\"Nc\"] = Nc.flatten(order=\"F\")\n",
    "            df[\"F\"] =   F.flatten(order=\"F\")\n",
    "            df[\"rho\"] = R.flatten(order=\"F\")\n",
    "            df[\"C\"] = np.repeat(concs, nsbtms)\n",
    "            \n",
    "            df.to_csv(\"{}/cplm/data/{}\".format(cwd, name), sep=\",\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation and saving of gLV data with and without adjustment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resource-consumer model for simulation\n",
    "\n",
    "def gLV(t, X, substrate_nb, gmix, Kix):\n",
    "    \n",
    "    S, N = X[:substrate_nb], X[substrate_nb:]\n",
    "    dX = np.zeros(X.shape)\n",
    "    dX[substrate_nb:] = gmix[:,0] * (S[0] * K_ix[:,0] - N) * N # dynamics of the population\n",
    "\n",
    "    return dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulation time\n",
    "\n",
    "dt = 0.1\n",
    "tmax = 50000\n",
    "subtime = 500 # timepoint subsampling: 1/500\n",
    "\n",
    "ts = np.arange(0, tmax + dt, dt) # timepoints for simulation\n",
    "subts = ts[::subtime,np.newaxis]\n",
    "T = np.repeat(subts, nconcs, axis=-1) # # timepoints for sub-sampling (ntimes, nconcs)\n",
    "N = np.zeros(T.shape) # sub-sampled population sizes (ntimes, nconcs)\n",
    "\n",
    "ntimes = ts.shape[0]\n",
    "nsbtms = T.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulation\n",
    "\n",
    "for resc in rescs[:1]:\n",
    "    \n",
    "    nu = nuxi[:resc,:]\n",
    "    gm = gmix[:,:resc]\n",
    "    K_ = K_ix[:,:resc]\n",
    "    \n",
    "    for strn in strns:\n",
    "                \n",
    "        name = \"{}s{}i{}.csv\".format(\"G\", 0, strn)\n",
    "            \n",
    "        for c, conc in enumerate(concs):\n",
    "            print(name, c, \"   \", end=\"\\r\")\n",
    "                \n",
    "            # intial condition\n",
    "            S0 = np.ones(resc) * 0.1 * conc #/ (resc)\n",
    "            N0 = np.zeros(nstrns)\n",
    "            N0[strn] = 0.1\n",
    "            X0 = np.concatenate([S0, N0])\n",
    "                \n",
    "            # Euler's method\n",
    "            for t, time in enumerate(ts):\n",
    "                dX = gLV(t, X0, resc, gm, K_,) * dt # Euler step\n",
    "                dX[resc:] *= alpha_RCM(time, qm_i[0], qm_i[1]) # adjustment function alpha\n",
    "                X0 += dX\n",
    "                # record only if the time is to be kept in subtimes Ts\n",
    "                if time in subts:\n",
    "                    N[np.argmax(subts==time),c] = np.sum(X0[resc:]) # (ntimes, nconcs)\n",
    "        \n",
    "        Nc = to_AUCs(T, N, t0=0)\n",
    "        F = np.exp(-Nc)\n",
    "        R = to_rhos(T, N)\n",
    "        \n",
    "        df = pd.DataFrame()\n",
    "        df[\"t\"] =   T.flatten(order=\"F\")\n",
    "        df[\"N\"] =   N.flatten(order=\"F\")\n",
    "        df[\"Nc\"] = Nc.flatten(order=\"F\")\n",
    "        df[\"F\"] =   F.flatten(order=\"F\")\n",
    "        df[\"rho\"] = R.flatten(order=\"F\")\n",
    "        df[\"C\"] = np.repeat(concs, nsbtms)\n",
    "        \n",
    "        df.to_csv(\"{}/cplm/data/{}\".format(cwd, name), sep=\",\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulation\n",
    "\n",
    "for resc in rescs[:1]:\n",
    "    \n",
    "    nu = nuxi[:resc,:]\n",
    "    gm = gmix[:,:resc]\n",
    "    K_ = K_ix[:,:resc]\n",
    "    \n",
    "    for strn in strns:\n",
    "                \n",
    "        name = \"{}s{}i{}.csv\".format(\"V\", 0, strn)\n",
    "            \n",
    "        for c, conc in enumerate(concs):\n",
    "            print(name, c, \"   \", end=\"\\r\")\n",
    "                \n",
    "            # intial condition\n",
    "            S0 = np.ones(resc) * 0.1 * conc #/ (resc)\n",
    "            N0 = np.zeros(nstrns)\n",
    "            N0[strn] = 0.1\n",
    "            X0 = np.concatenate([S0, N0])\n",
    "                \n",
    "            # Euler's method\n",
    "            for t, time in enumerate(ts):\n",
    "                dX = gLV(t, X0, resc, gm, K_,) * dt # Euler step\n",
    "#                dX[resc:] *= alpha_RCM(time, qm_i[0], qm_i[1]) # adjustment function alpha\n",
    "                X0 += dX\n",
    "                # record only if the time is to be kept in subtimes Ts\n",
    "                if time in subts:\n",
    "                    N[np.argmax(subts==time),c] = np.sum(X0[resc:]) # (ntimes, nconcs)\n",
    "        \n",
    "        Nc = to_AUCs(T, N, t0=0)\n",
    "        F = np.exp(-Nc)\n",
    "        R = to_rhos(T, N)\n",
    "        \n",
    "        df = pd.DataFrame()\n",
    "        df[\"t\"] =   T.flatten(order=\"F\")\n",
    "        df[\"N\"] =   N.flatten(order=\"F\")\n",
    "        df[\"Nc\"] = Nc.flatten(order=\"F\")\n",
    "        df[\"F\"] =   F.flatten(order=\"F\")\n",
    "        df[\"rho\"] = R.flatten(order=\"F\")\n",
    "        df[\"C\"] = np.repeat(concs, nsbtms)\n",
    "        \n",
    "        df.to_csv(\"{}/cplm/data/{}\".format(cwd, name), sep=\",\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
